{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61e7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main module is loaded\n",
      "/home/rzhang98/code2025/CIBMTR_post_hct_survival/scripts/main_module.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import main_module as md\n",
    "\n",
    "# figure fonts configuration\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7444601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import set_config\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "# import the score function\n",
    "%run -i ../examples/concordance_index.ipynb\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1d6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_test= pd.read_csv(\"../data/test_validation_set.csv\")\n",
    "df_train = pd.read_csv(\"../data/train_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba1f77",
   "metadata": {},
   "source": [
    "## Data preprocessing pipelines\n",
    "In this section, we create pipelines for preprocessing the data. The main goal here is to investigate if data imputation improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d114a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive preprocessor\n",
    "# replace missing categorical variables by 'missing', replace missing numerical values by -1\n",
    "class NaiveDataTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transform = X.copy(deep = True)\n",
    "        cat_cols = X_transform.select_dtypes(include = 'O').columns\n",
    "        num_cols = X_transform.select_dtypes(exclude = 'O').columns\n",
    "        X_transform[cat_cols] = X_transform[cat_cols].fillna(\"missing\")\n",
    "        X_transform[num_cols] = X_transform[num_cols].fillna(-1.0)\n",
    "        return X_transform\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return self \n",
    "    \n",
    "    def get_feature_names_out(self, input_features = None):\n",
    "        return self.columns\n",
    "\n",
    "cat_cols = df_train.select_dtypes(include='O').columns\n",
    "num_cols = df_train.select_dtypes(exclude='O').columns.drop([\"ID\", 'year_hct','efs', 'efs_time'])\n",
    "other_cols = df_train.columns.drop([\"ID\", 'year_hct','efs', 'efs_time'])\n",
    "# set_config(transform_output=\"pandas\")\n",
    "preproc_naive = Pipeline(\n",
    "    steps = [('preprocessing',\n",
    "                ColumnTransformer([('naive_missing', NaiveDataTransformer(), other_cols),\n",
    "                                ('ID_year_dropper', 'drop', [\"ID\", 'year_hct'])],\n",
    "                                    sparse_threshold=0,\n",
    "                                    remainder='passthrough',\n",
    "                                    verbose_feature_names_out=False,\n",
    "                                    force_int_remainder_cols=False\n",
    "                                ).set_output(transform=\"pandas\")\n",
    "            ),\n",
    "            ('naive_one_hot_encode',\n",
    "                ColumnTransformer([('one_hot', OneHotEncoder(drop='first',\n",
    "                                                             min_frequency = 0.001,\n",
    "                                                             handle_unknown='ignore',\n",
    "                                                             sparse_output= False), cat_cols)],\n",
    "                                    sparse_threshold=0,\n",
    "                                    remainder='passthrough',\n",
    "                                    verbose_feature_names_out=False,\n",
    "                                    force_int_remainder_cols=False\n",
    "                                ).set_output(transform=\"pandas\")\n",
    "            )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5452c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing based on KNN imputation \n",
    "class MissingValueTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, null_list = [\"Missing Disease Status\", \"Missing disease status\"]):\n",
    "        self.null_list = null_list\n",
    "        self.columns = None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transform = X.copy(deep = True)\n",
    "        X_transform.replace(self.null_list, np.nan, inplace = True)\n",
    "        return X_transform\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return self \n",
    "    \n",
    "    def get_feature_names_out(self, input_features = None):\n",
    "        return self.columns\n",
    "\n",
    "cat_cols = df_train.select_dtypes(include='O').columns\n",
    "preproc_sd = Pipeline(\n",
    "    [   \n",
    "        ('preprocessing',\n",
    "                ColumnTransformer([\n",
    "                                    ('cat_missing', MissingValueTransformer(), cat_cols),\n",
    "                                    ('ID_year_dropper', 'drop', [\"ID\", 'year_hct'])],\n",
    "                                    sparse_threshold=0,\n",
    "                                    remainder='passthrough',\n",
    "                                    verbose_feature_names_out=False,\n",
    "                                    force_int_remainder_cols=False\n",
    "                                ).set_output(transform=\"pandas\")\n",
    "        ),\n",
    "        (\n",
    "            \"encode_and_scale\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    ('one_hot', \n",
    "                    OneHotEncoder(drop='first',\n",
    "                                    min_frequency = 0.001,\n",
    "                                    handle_unknown='ignore',\n",
    "                                    sparse_output= False\n",
    "                    ), \n",
    "                    cat_cols\n",
    "                    ),\n",
    "                    ('scale', StandardScaler(), ['donor_age', 'age_at_hct', 'karnofsky_score'])\n",
    "                ],\n",
    "                sparse_threshold=0,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False,\n",
    "                force_int_remainder_cols=False\n",
    "            ).set_output(transform=\"pandas\")\n",
    "        ),\n",
    "        (\n",
    "            \"impute\",\n",
    "            KNNImputer().set_output(transform = \"pandas\")\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82651a10",
   "metadata": {},
   "source": [
    "## Modeling Methods\n",
    "\n",
    "In this section, we implement the actual modeling methods including \n",
    "* CoxPH model\n",
    "* XGboost AFT\n",
    "* Catboost AFT\n",
    "* Survival Random Foreast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713d495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cox propotional harzard model\n",
    "def cph_model(X_train_preproc, y_train, X_test_preproc):\n",
    "\n",
    "    train_preproc = pd.concat([X_train_preproc, y_train], axis=1)\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(train_preproc, duration_col='efs_time', event_col='efs')\n",
    "    preds = cph.predict_partial_hazard(X_test_preproc)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ddfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGboost\n",
    "params = {'objective': 'survival:aft',\n",
    "          'eval_metric': 'aft-nloglik',\n",
    "          'aft_loss_distribution': 'normal',\n",
    "         'aft_loss_distribution_scale': 0.80,\n",
    "          'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 6}\n",
    "   \n",
    "def xgb_aft_model(X_train_preproc, y_train, X_test_preproc, params = params):\n",
    "\n",
    "    # remove special character\n",
    "    X_train_preproc.columns = X_train_preproc.columns.str.replace('<','')\n",
    "    X_test_preproc.columns = X_test_preproc.columns.str.replace('<','')\n",
    "\n",
    "    y_lower_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound[y_train['efs'] == 0.0] = +np.inf\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_preproc)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower_bound)\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper_bound)\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=500, evals=[(dtrain, 'train')], verbose_eval = 0)\n",
    "    dtest = xgb.DMatrix(X_test_preproc)\n",
    "    preds = bst.predict(dtest)\n",
    "\n",
    "    return -preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7b7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Catboost\n",
    "\n",
    "# catboost does not directly take one-hot encoding\n",
    "# instead, it requires an explicit declaration of catgorical features\n",
    "\n",
    "# Here are the modified pipelines for catboost\n",
    "# The basic idea is to remove the one-hot encoding process\n",
    "\n",
    "num_cols = df_train.select_dtypes(exclude='O').columns.drop([\"ID\", 'year_hct','efs', 'efs_time'])\n",
    "cb_preproc_naive = NaiveDataTransformer()\n",
    "cb_preproc_sd = Pipeline(\n",
    "    [   \n",
    "        ('preprocessing',\n",
    "                ColumnTransformer([\n",
    "                                    ('cat_missing', MissingValueTransformer(), cat_cols),\n",
    "                                    ('ID_year_dropper', 'drop', [\"ID\", 'year_hct']),\n",
    "                                    ('scale', StandardScaler(), ['donor_age', 'age_at_hct', 'karnofsky_score'])],\n",
    "                                    sparse_threshold=0,\n",
    "                                    remainder='passthrough',\n",
    "                                    verbose_feature_names_out=False,\n",
    "                                    force_int_remainder_cols=False\n",
    "                                ).set_output(transform=\"pandas\")\n",
    "        ),\n",
    "        ('impute',\n",
    "                ColumnTransformer([(\"num_KNNimpute\", KNNImputer(), num_cols),\n",
    "                                   (\"cat_indicate\", NaiveDataTransformer(), cat_cols)],\n",
    "                                    sparse_threshold=0,\n",
    "                                    remainder='passthrough',\n",
    "                                    verbose_feature_names_out=False,\n",
    "                                    force_int_remainder_cols=False\n",
    "                                ).set_output(transform=\"pandas\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def cb_aft_model(X_train, y_train, X_test, y_test, pipeline):\n",
    "\n",
    "    X_train_proc = pipeline.fit_transform(X_train)\n",
    "    X_test_proc = pipeline.fit_transform(X_test)\n",
    "\n",
    "    y_lower_train = y_train[['efs_time']].copy(deep = True)\n",
    "    y_upper_train = y_train[['efs_time']].copy(deep = True)\n",
    "    # in catboost, infinity is represented by -1\n",
    "    y_upper_train.iloc[y_train['efs'] == 0.0] = -1\n",
    "\n",
    "    train_label = np.concatenate((y_lower_train, y_upper_train), axis = 1)\n",
    "    train_label = pd.DataFrame(train_label, columns = ['y_lower_train', 'y_upper_train'])\n",
    "    cat_features = list(X_train.select_dtypes(include= 'O').columns)\n",
    "\n",
    "    y_lower_test = y_test[['efs_time']].copy(deep = True)\n",
    "    y_upper_test = y_test[['efs_time']].copy(deep = True)\n",
    "    # in catboost, infinity is represented by -1\n",
    "    y_upper_test.iloc[y_test['efs'] == 0.0] = -1\n",
    "    \n",
    "    test_label = np.concatenate((y_lower_test, y_upper_test), axis = 1)\n",
    "    test_label = pd.DataFrame(test_label, columns = ['y_lower_test', 'y_upper_test'])\n",
    "\n",
    "    train_pool = Pool(X_train_proc,label = train_label, cat_features= cat_features)\n",
    "    test_pool = Pool(X_test_proc,label = test_label, cat_features= cat_features)\n",
    "\n",
    "    model_normal = CatBoostRegressor(iterations=500,\n",
    "                                 loss_function='SurvivalAft:dist=Normal',\n",
    "                                 eval_metric='SurvivalAft',\n",
    "                                 verbose=0\n",
    "                                )\n",
    "    _ = model_normal.fit(train_pool, eval_set=test_pool)\n",
    "    preds = model_normal.predict(test_pool, prediction_type='Exponent')\n",
    "    \n",
    "    return -preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random survival forest\n",
    "def rsf_model(X_train_preproc, y_train, X_test_preproc):\n",
    "\n",
    "    y_train = Surv.from_dataframe(\"efs\", \"efs_time\", y_train)\n",
    "\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        n_jobs=4,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rsf.fit(X_train_preproc, y_train)\n",
    "    surv_funcs = rsf.predict_survival_function(X_test_preproc, return_array=False)\n",
    "    preds = np.array([-np.trapz(fn.y, fn.x) for fn in surv_funcs])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700e40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hybrid cph\n",
    "\n",
    "def hybrid_cph_model(X_train_preproc, y_train, X_test_preproc):\n",
    "\n",
    "    # create embedding with logistic regression\n",
    "    train_idx = X_train_preproc.index\n",
    "    test_idx = X_test_preproc.index\n",
    "    col = X_train_preproc.columns.to_list()\n",
    "    col.append(\"class\")\n",
    "    clf = LogisticRegression(max_iter=15000)\n",
    "    clf.fit(X_train_preproc, y_train[\"efs\"])\n",
    "    X_train_preproc = pd.DataFrame(data = np.concatenate((X_train_preproc.to_numpy(), np.reshape(clf.predict(X_train_preproc), (-1, 1) )), axis=1), \n",
    "                                   index = train_idx,\n",
    "                                   columns = col)\n",
    "\n",
    "    X_test_preproc = pd.DataFrame(data = np.concatenate((X_test_preproc.to_numpy(), np.reshape(clf.predict(X_test_preproc), (-1, 1) )), axis=1), \n",
    "                                  index = test_idx,\n",
    "                                  columns = col)\n",
    "\n",
    "    # train cph\n",
    "    train_preproc = pd.concat([X_train_preproc, y_train], axis=1)\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(train_preproc, duration_col='efs_time', event_col='efs')\n",
    "    preds = cph.predict_partial_hazard(X_test_preproc)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7329eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hybrid XGboost\n",
    "\n",
    "params = {'objective': 'survival:aft',\n",
    "          'eval_metric': 'aft-nloglik',\n",
    "          'aft_loss_distribution': 'normal',\n",
    "         'aft_loss_distribution_scale': 0.80,\n",
    "          'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 6}\n",
    "   \n",
    "def hybrid_xgb_aft_model(X_train_preproc, y_train, X_test_preproc, params = params):\n",
    "\n",
    "    # create embedding with logistic regression\n",
    "    train_idx = X_train_preproc.index\n",
    "    test_idx = X_test_preproc.index\n",
    "    col = X_train_preproc.columns.to_list()\n",
    "    col.append(\"class\")\n",
    "    clf = LogisticRegression(max_iter=15000)\n",
    "    clf.fit(X_train_preproc, y_train[\"efs\"])\n",
    "    X_train_preproc = pd.DataFrame(data = np.concatenate((X_train_preproc.to_numpy(), np.reshape(clf.predict(X_train_preproc), (-1, 1) )), axis=1), \n",
    "                                   index = train_idx,\n",
    "                                   columns = col)\n",
    "\n",
    "    X_test_preproc = pd.DataFrame(data = np.concatenate((X_test_preproc.to_numpy(), np.reshape(clf.predict(X_test_preproc), (-1, 1) )), axis=1), \n",
    "                                  index = test_idx,\n",
    "                                  columns = col)\n",
    "\n",
    "    # train XGboost\n",
    "    # remove special character\n",
    "    X_train_preproc.columns = X_train_preproc.columns.str.replace('<','')\n",
    "    X_test_preproc.columns = X_test_preproc.columns.str.replace('<','')\n",
    "\n",
    "    y_lower_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound[y_train['efs'] == 0.0] = +np.inf\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_preproc)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower_bound)\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper_bound)\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=500, evals=[(dtrain, 'train')], verbose_eval = 0)\n",
    "    dtest = xgb.DMatrix(X_test_preproc)\n",
    "    preds = bst.predict(dtest)\n",
    "\n",
    "    return -preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e63ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hybrid random survival forest\n",
    "\n",
    "def hybrid_rsf_model(X_train_preproc, y_train, X_test_preproc):\n",
    "\n",
    "    # create embedding with logistic regression\n",
    "    train_idx = X_train_preproc.index\n",
    "    test_idx = X_test_preproc.index\n",
    "    col = X_train_preproc.columns.to_list()\n",
    "    col.append(\"class\")\n",
    "    clf = LogisticRegression(max_iter=15000)\n",
    "    clf.fit(X_train_preproc, y_train[\"efs\"])\n",
    "    X_train_preproc = pd.DataFrame(data = np.concatenate((X_train_preproc.to_numpy(), np.reshape(clf.predict(X_train_preproc), (-1, 1) )), axis=1), \n",
    "                                   index = train_idx,\n",
    "                                   columns = col)\n",
    "\n",
    "    X_test_preproc = pd.DataFrame(data = np.concatenate((X_test_preproc.to_numpy(), np.reshape(clf.predict(X_test_preproc), (-1, 1) )), axis=1), \n",
    "                                  index = test_idx,\n",
    "                                  columns = col)\n",
    "\n",
    "    # train random survival forest\n",
    "    y_train = Surv.from_dataframe(\"efs\", \"efs_time\", y_train)\n",
    "\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        n_jobs=4,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rsf.fit(X_train_preproc, y_train)\n",
    "    surv_funcs = rsf.predict_survival_function(X_test_preproc, return_array=False)\n",
    "    preds = np.array([-np.trapz(fn.y, fn.x) for fn in surv_funcs])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69362b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hybird rf-xgb\n",
    "\n",
    "class RFCTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "        self.rcf_classifier = None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transform = X.copy(deep = True)\n",
    "        proba_column = self.rcf_classifier.predict_proba(X_transform)[:, 1]\n",
    "        X_transform[\"clf_proba\"] = proba_column\n",
    "\n",
    "        self.columns = X_transform.columns\n",
    "        return X_transform\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        RFC = RandomForestClassifier()\n",
    "        RFC.fit(X, y)\n",
    "        self.rcf_classifier = RFC\n",
    "        self._is_fitted = True\n",
    "        return self \n",
    "    \n",
    "    def get_feature_names_out(self, input_features = None):\n",
    "        return self.columns\n",
    "\n",
    "    def __sklearn_is_fitted__(self):\n",
    "        return hasattr(self, \"_is_fitted\") and self._is_fitted\n",
    "\n",
    "cat_cols = df_train.select_dtypes(include='O').columns\n",
    "num_cols = df_train.select_dtypes(exclude='O').columns.drop([\"ID\", 'year_hct', 'efs', 'efs_time'])\n",
    "\n",
    "# Ela - pipeline\n",
    "preproc_ela = Pipeline(steps=[\n",
    "    ('preprocessor', MissingValueTransformer(\n",
    "        null_list = [\"Not done\", \"Not tested\", \"Other\", \"Missing disease status\", \"Non-resident of the U.S.\"])\n",
    "    ),\n",
    "    ('imputing one-hot encoding',\n",
    "        ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat_imputer', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                        ('one-hot encoder', OneHotEncoder(drop='first', \n",
    "                                                                    sparse_output=False, \n",
    "                                                                    handle_unknown='ignore'))]), cat_cols),\n",
    "                ('num_imputer', Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
    "                                        ('scaler', StandardScaler())]), num_cols),\n",
    "                ('ID_year_dropper', 'drop', [\"ID\", 'year_hct'])\n",
    "            ],\n",
    "            sparse_threshold=0,\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False,\n",
    "            force_int_remainder_cols=False\n",
    "        ).set_output(transform = 'pandas')\n",
    "    ),\n",
    "    ('random forest transform', RFCTransformer().set_output(transform = 'pandas'))\n",
    "])\n",
    "\n",
    "def rf_xgb(X_train, y_train, X_test, y_test):\n",
    "    params = {\n",
    "        'objective': 'survival:aft',\n",
    "        'eval_metric': 'aft-nloglik',\n",
    "        'tree_method': 'hist',\n",
    "        'seed': 42, \n",
    "        'aft_loss_distribution': 'normal',\n",
    "        'aft_loss_distribution_scale': 0.34089400351953153,\n",
    "        'learning_rate': 0.07894387344725944,\n",
    "        'max_depth': 8,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.947394577078348,\n",
    "        'colsample_bytree': 0.8323203114860168,\n",
    "        'lambda': 0.40756304508622526,\n",
    "        'alpha': 6.828765311809384\n",
    "    }\n",
    "    preproc_ela.fit(X_train, y_train['efs'])\n",
    "    X_train_preproc = preproc_ela.transform(X_train)\n",
    "    X_test_preproc = preproc_ela.transform(X_test)\n",
    "\n",
    "    # remove special character\n",
    "    X_train_preproc.columns = X_train_preproc.columns.str.replace('<','')\n",
    "    X_test_preproc.columns = X_test_preproc.columns.str.replace('<','')\n",
    "\n",
    "    y_lower_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound = y_train['efs_time'].copy(deep = True)\n",
    "    y_upper_bound[y_train['efs'] == 0.0] = +np.inf\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_preproc)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower_bound)\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper_bound)\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=500, evals=[(dtrain, 'train')], verbose_eval = 0)\n",
    "    dtest = xgb.DMatrix(X_test_preproc)\n",
    "    preds = bst.predict(dtest)\n",
    "\n",
    "    return -preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ecc78",
   "metadata": {},
   "source": [
    "## Five-fold Cross validation\n",
    "\n",
    "Here we use the five fold cross validation for getting a baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859d8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# eval evalutes stratified c-index and c-index\n",
    "def eval(preds, X_test, solution):\n",
    "    prediction= pd.DataFrame({\"ID\":X_test[\"ID\"], \"prediction\":preds})\n",
    "    sc_score = score(solution.copy(deep=True), prediction.copy(deep=True), \"ID\")\n",
    "    c_index = concordance_index(y_test['efs_time'], -preds, y_test['efs'])\n",
    "\n",
    "    return sc_score, c_index\n",
    "\n",
    "# file_output export all the information into a csv file\n",
    "def file_output(filename, sc_indexes, columns):\n",
    "    sc_mean = sc_indexes.mean()\n",
    "    output = pd.DataFrame(np.concatenate(\n",
    "                                        (sc_indexes.to_numpy(), np.expand_dims(sc_mean.to_numpy(), axis = 0)\n",
    "                                     ), axis = 0), \n",
    "                                     index=[0,1,2,3,4, 'mean'],\n",
    "                                     columns = columns)\n",
    "    output.to_csv(filename, sep= '\\t', index= True, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61f5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   14.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 0: \n",
      "                 SC-index: cph: 0.652276376889573, xgb_aft: 0.6584019240108607, cat_aft: 0.6587152664116722, rsf_aft: 0.6251971163193465 \n",
      "                 C_index: cph: 0.67454908235802, xgb_aft: 0.6760540015952347, cat_aft: 0.6821519360158316, rsf_aft: 0.6488515014630147 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   15.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   18.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 1: \n",
      "                 SC-index: cph: 0.6443178893129566, xgb_aft: 0.6392215188149583, cat_aft: 0.6478186882133291, rsf_aft: 0.6212123949052654 \n",
      "                 C_index: cph: 0.6679025860424361, xgb_aft: 0.6607663101337165, cat_aft: 0.67152380373496, rsf_aft: 0.6429413878124829 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rzhang98/miniconda3/envs/boost_env/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [25] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   15.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 2: \n",
      "                 SC-index: cph: 0.6619024320614599, xgb_aft: 0.6636890474633307, cat_aft: 0.6674275566820878, rsf_aft: 0.6369712769727391 \n",
      "                 C_index: cph: 0.6739880730672791, xgb_aft: 0.6742245332664779, cat_aft: 0.6804133252445986, rsf_aft: 0.6498288447201191 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   18.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 3: \n",
      "                 SC-index: cph: 0.6426435877769467, xgb_aft: 0.6461064348430947, cat_aft: 0.650319985544931, rsf_aft: 0.616889016685384 \n",
      "                 C_index: cph: 0.6587793129897566, xgb_aft: 0.6639460815488878, cat_aft: 0.666539994984622, rsf_aft: 0.6312815014854256 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   15.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   21.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 4: \n",
      "                 SC-index: cph: 0.652361726219943, xgb_aft: 0.6562894348770086, cat_aft: 0.6646352343262615, rsf_aft: 0.6226528966871737 \n",
      "                 C_index: cph: 0.669041538178832, xgb_aft: 0.672468777695469, cat_aft: 0.6761054978437013, rsf_aft: 0.6457909825945641 \n",
      "\n",
      "mean stratified c-index across all 5 folds:\n",
      "cph          0.650700\n",
      "h_cph        0.651429\n",
      "xgb_aft      0.652742\n",
      "h_xgb_aft    0.653336\n",
      "rsf          0.624585\n",
      "h_rsf        0.633964\n",
      "cat_aft      0.657783\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "preproc_pipline = preproc_naive\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits = n_splits, shuffle = True, random_state = 42)\n",
    "target_features = ['efs', 'efs_time']\n",
    "\n",
    "methods_list = ['cph', 'h_cph', 'xgb_aft', 'h_xgb_aft', 'rsf', 'h_rsf', 'cat_aft']\n",
    "sc_indexes = -1.0 * np.ones((n_splits, len(methods_list))) \n",
    "sc_indexes = pd.DataFrame(data = sc_indexes, columns = methods_list)\n",
    "\n",
    "for i, (train_idx,test_idx) in enumerate(kfold.split(df_train)):\n",
    "        X_train = df_train.iloc[train_idx].drop(columns = target_features)\n",
    "        y_train = df_train.loc[train_idx, target_features]\n",
    "\n",
    "        X_test = df_train.iloc[test_idx].drop(columns = target_features)\n",
    "        y_test = df_train.loc[test_idx, target_features]\n",
    "\n",
    "        preproc_pipline.fit(X_train)\n",
    "        X_train_preproc = preproc_pipline.transform(X_train)\n",
    "        X_test_preproc =preproc_pipline.transform(X_test)\n",
    "\n",
    "        solution = df_train.iloc[test_idx]\n",
    "\n",
    "        # Chi-Hao logistic hybrid versions\n",
    "        # hybrid cph\n",
    "        preds_hcph = hybrid_cph_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hcph, c_index_hcph = eval(preds_hcph, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_cph'] = score_hcph\n",
    "\n",
    "        # hybrid XGB\n",
    "        preds_hxgb = hybrid_xgb_aft_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hxgb, c_index_hxgb = eval(preds_hxgb, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_xgb_aft'] = score_hxgb\n",
    "\n",
    "        # hybrid survival random forest\n",
    "        preds_hrsf = hybrid_rsf_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hrsf, c_index_hrsf = eval(preds_hrsf, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_rsf'] = score_hrsf\n",
    "\n",
    "        preds_cph = cph_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        preds_xgb = xgb_aft_model(X_train_preproc, y_train, X_test_preproc, params = params)\n",
    "        preds_cb = cb_aft_model(X_train, y_train, X_test, y_test, cb_preproc_naive)\n",
    "        preds_rsf = rsf_model(X_train_preproc, y_train, X_test_preproc)\n",
    "\n",
    "        score_cph, c_index_cph = eval(preds_cph, X_test, solution)\n",
    "        score_xgb, c_index_xgb = eval(preds_xgb, X_test, solution)\n",
    "        score_cb, c_index_cb = eval(preds_cb, X_test, solution)\n",
    "        score_rsf, c_index_rsf = eval(preds_rsf, X_test, solution)\n",
    "\n",
    "        print(f\"stratified c-index for fold {i}: \\n \\\n",
    "                SC-index: cph: {score_cph}, xgb_aft: {score_xgb}, cat_aft: {score_cb}, rsf_aft: {score_rsf} \\n \\\n",
    "                C_index: cph: {c_index_cph}, xgb_aft: {c_index_xgb}, cat_aft: {c_index_cb}, rsf_aft: {c_index_rsf} \\n\")\n",
    "\n",
    "        sc_indexes.loc[i, 'cph'] = score_cph\n",
    "        sc_indexes.loc[i, 'xgb_aft'] = score_xgb\n",
    "        sc_indexes.loc[i, 'cat_aft'] = score_cb\n",
    "        sc_indexes.loc[i, 'rsf'] = score_rsf\n",
    "\n",
    "print(\"mean stratified c-index across all 5 folds:\")\n",
    "print(sc_indexes.mean())\n",
    "\n",
    "file_output('output_naive(baseline)_cv.csv', sc_indexes, methods_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab26728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 0: \n",
      "                 SC-index: cph: 0.6546097516619727, xgb_aft: 0.6528320804213839,  cat_aft: 0.6543832636607791, rsf_aft: 0.6260525341646062 \n",
      "                 C_index: cph: 0.6750910944429793, xgb_aft: 0.6709875615389657, cat_aft: 0.6756497011710685, rsf_aft: 0.6476231397136433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 1: \n",
      "                 SC-index: cph: 0.6440651847196557, xgb_aft: 0.6423926885764277,  cat_aft: 0.6418738779362735, rsf_aft: 0.6191700445422199 \n",
      "                 C_index: cph: 0.6692466709315013, xgb_aft: 0.6625115021144633, cat_aft: 0.6657436489371114, rsf_aft: 0.641404241925651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rzhang98/miniconda3/envs/boost_env/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [25] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/home/rzhang98/miniconda3/envs/boost_env/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [25] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 2: \n",
      "                 SC-index: cph: 0.6612368552262808, xgb_aft: 0.6585458563856175,  cat_aft: 0.6659247025421783, rsf_aft: 0.6327720678215303 \n",
      "                 C_index: cph: 0.6738423656938993, xgb_aft: 0.6730358867562773, cat_aft: 0.6776689502136942, rsf_aft: 0.644917117407697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 3: \n",
      "                 SC-index: cph: 0.6464834164525809, xgb_aft: 0.6476285379910794,  cat_aft: 0.6436857315381221, rsf_aft: 0.6177262659067099 \n",
      "                 C_index: cph: 0.6605217827424174, xgb_aft: 0.6607113678924575, cat_aft: 0.660917003217517, rsf_aft: 0.6295392730887801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified c-index for fold 4: \n",
      "                 SC-index: cph: 0.6502967729721472, xgb_aft: 0.6457667910653513,  cat_aft: 0.6572208978276771, rsf_aft: 0.6229223104806936 \n",
      "                 C_index: cph: 0.6695021323716606, xgb_aft: 0.6678246722483505, cat_aft: 0.6721087365985036, rsf_aft: 0.6448617786736036\n",
      "Ray pipeline mean stratified c-index across all 5 folds:\n",
      "cph          0.651338\n",
      "h_cph        0.652316\n",
      "xgb_aft      0.649433\n",
      "h_xgb_aft    0.649548\n",
      "rf_xgb       0.631645\n",
      "rsf          0.623729\n",
      "h_rsf        0.631653\n",
      "cat_aft      0.652618\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "preproc_pipline = preproc_sd\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits = n_splits, shuffle = True, random_state = 42)\n",
    "target_features = ['efs', 'efs_time']\n",
    "\n",
    "methods_list = ['cph', 'h_cph', 'xgb_aft', 'h_xgb_aft', 'rf_xgb', 'rsf', 'h_rsf', 'cat_aft']\n",
    "sc_indexes = -1.0 * np.ones((n_splits, len(methods_list))) \n",
    "sc_indexes = pd.DataFrame(data = sc_indexes, columns = methods_list)\n",
    "\n",
    "try:\n",
    "    for i, (train_idx,test_idx) in enumerate(kfold.split(df_train)):\n",
    "\n",
    "        X_train = df_train.iloc[train_idx].drop(columns = target_features)\n",
    "        y_train = df_train.loc[train_idx, target_features]\n",
    "\n",
    "        X_test = df_train.iloc[test_idx].drop(columns = target_features)\n",
    "        y_test = df_train.loc[test_idx, target_features]\n",
    "\n",
    "        preproc_pipline.fit(X_train)\n",
    "        X_train_preproc = preproc_pipline.transform(X_train)\n",
    "        X_test_preproc =preproc_pipline.transform(X_test)\n",
    "\n",
    "        solution = df_train.iloc[test_idx]\n",
    "\n",
    "        # Ela random forest hybrid XGB\n",
    "        preds_rf_xgb = rf_xgb(X_train, y_train, X_test, y_test)\n",
    "        score_rf_xgb, c_index_rf_xgb = eval(preds_rf_xgb, X_test, solution)\n",
    "        sc_indexes.loc[i, 'rf_xgb'] = score_rf_xgb\n",
    "\n",
    "        # Chi-Hao logistic hybrid versions\n",
    "        # hybrid cph\n",
    "        preds_hcph = hybrid_cph_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hcph, c_index_hcph = eval(preds_hcph, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_cph'] = score_hcph\n",
    "\n",
    "        # hybrid XGB\n",
    "        preds_hxgb = hybrid_xgb_aft_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hxgb, c_index_hxgb = eval(preds_hxgb, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_xgb_aft'] = score_hxgb\n",
    "\n",
    "        # hybrid survival random forest\n",
    "        preds_hrsf = hybrid_rsf_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_hrsf, c_index_hrsf = eval(preds_hrsf, X_test, solution)\n",
    "        sc_indexes.loc[i, 'h_rsf'] = score_hrsf\n",
    "\n",
    "        # Yang/Ray survival random forest\n",
    "        preds_rsf = rsf_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_rsf, c_index_rsf = eval(preds_rsf, X_test, solution)\n",
    "        sc_indexes.loc[i, 'rsf'] = score_rsf\n",
    "\n",
    "        # Ray baseline Cox proportional harzard\n",
    "        preds_cph = cph_model(X_train_preproc, y_train, X_test_preproc)\n",
    "        score_cph, c_index_cph = eval(preds_cph, X_test, solution)\n",
    "        sc_indexes.loc[i, 'cph'] = score_cph\n",
    "\n",
    "        # Ela/Ruibo xgb aft\n",
    "        preds_xgb = xgb_aft_model(X_train_preproc, y_train, X_test_preproc, params = params)\n",
    "        score_xgb, c_index_xgb = eval(preds_xgb, X_test, solution)\n",
    "        sc_indexes.loc[i, 'xgb_aft'] = score_xgb\n",
    "\n",
    "        # Ruibo catboost aft\n",
    "        preds_cb = cb_aft_model(X_train, y_train, X_test, y_test, cb_preproc_sd)\n",
    "        score_cb, c_index_cb = eval(preds_cb, X_test, solution)\n",
    "        sc_indexes.loc[i, 'cat_aft'] = score_cb\n",
    "\n",
    "        print(f\"stratified c-index for fold {i}: \\n \\\n",
    "                SC-index: cph: {score_cph}, xgb_aft: {score_xgb},  cat_aft: {score_cb}, rsf_aft: {score_rsf} \\n \\\n",
    "                C_index: cph: {c_index_cph}, xgb_aft: {c_index_xgb}, cat_aft: {c_index_cb}, rsf_aft: {c_index_rsf}\")\n",
    "    print(\"Ray pipeline mean stratified c-index across all 5 folds:\")\n",
    "    print(sc_indexes.mean())\n",
    "\n",
    "    file_output('output_proc(Ray)_cv.csv', sc_indexes, methods_list)\n",
    "except:\n",
    "    print(\"failed\")\n",
    "    file_output('output_proc(Ray)_cv(ERROR).csv', sc_indexes, methods_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boost_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
